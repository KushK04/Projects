# -*- coding: utf-8 -*-
"""Marham webscrape1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyeq3vV-YUhfii7qP2tpqyaiEBIOABt2

https://www.marham.pk/forum
TO-DO:
- Convert reply structure  to [[q1, reply1, reply2], [q2, reply1, reply2]]
- Filter out answers that are "make appointment", either mark them, add them to the end of the list, or both
"""

from collections import Counter
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
import json

totalPostCounter = 0

#Information on the classes we need
link = "https://www.marham.pk/forum?page=" #Generic link for forum
classOP = "card-body page-title-content padding" #class for original post
classPosts = "feed-section-link" #class for post links
classReply = "col-10 col-md-11" #class for replies
classReply2 = "row topmargin-lg"
classLinkPic = "col-md-12 cz-carousel cz-controls-static cz-controls-outside p-0"#class for images
classTitle = "breadcrumb-item active" #class for titles
classFirstPost = "col-12 text-justify" #class for the first post text
classOUser = "col-9 label-patient-info" #class for original user
classUsers =  "label-user" #class for all user ids


data = {}
linkList = [] #List of topic links
appointmentList = [] #list of links with make appoint ent as answer
totalPost = 0
titleSet = set()
titlePost = {}
imgCount = 0

print("Gathering Links")

#Gets the links in linkList
start = 1
end = 30
for y in range(start,end): #30 mins for 100
  print(y, " / ", end)
  linkNew = link + str(y)
  linkRequest = requests.get(linkNew)
  linkSoup = BeautifulSoup(linkRequest.text,'lxml')
  links = linkSoup.find_all('a', href=True, class_=classPosts)
  for x in links:
    totalPostCounter += 1
    linkRequestPrev = requests.get(x['href'])
    linkSoupPrev = BeautifulSoup(linkRequestPrev.text,'lxml')
    if(linkSoupPrev.find(class_=classLinkPic)):
      print("yay")
      firstReplyCheck = linkSoupPrev.find('div', {'class':[classReply]})
      if firstReplyCheck:
        firstReplyText = firstReplyCheck.find_all('p')[-3].text.strip().lower()
        if "appointment" in firstReplyText or "consultation" in firstReplyText:
          appointmentList.append(x['href'] + "?page=")
        else:
          linkList.append(x['href'] + "?page=")
      else:
        linkList.append(x['href'] + "?page=")

print(len(linkList))
print(len(appointmentList))

print("Iterating through Links")
newList = linkList + appointmentList

allCounter = 0
postsCounter = 0
answersCounter = 0
for x in newList: #Iterating through each webpage
  currentLink = newList.index(x)
  print(currentLink, " / ", len(newList))
  allCounter += 1
  postsCounter += 1
  answersCounter = 0
  linkRequest = requests.get(x)
  linkSoup = BeautifulSoup(linkRequest.text,'lxml')
  titles = linkSoup.find(class_=classTitle).text.replace('\n','').strip()
  firstPost = linkSoup.find(class_=classFirstPost).find_all('p')
  firstPostUser = linkSoup.find(class_=classOUser).text.strip()
  firstPostInfo = {}
  firstPostIMG = linkSoup.find(class_=classLinkPic).find_all('img')
  firstPostInfo["Thread-ID"] = str(postsCounter) + "-" + str(answersCounter)
  firstPostInfo["URL"] = x
  firstPostInfo["Title"] = titles
  titleSet.add(firstPostInfo["Title"])
  if (firstPostInfo["Title"] in titlePost.keys()):
    titlePost[firstPostInfo["Title"]] += 1
  else:
    titlePost[firstPostInfo["Title"]] = 1
  firstPostInfo["User-ID"] = firstPostUser
  firstPostTexts = []
  for p in firstPost:
      textToAdd = p.text.strip()
      if len(textToAdd) > 0:
        firstPostTexts.append(textToAdd)
        totalPost += len(textToAdd.split())
  firstPostInfo["Text"] = firstPostTexts
  firstPostIMGS = []
  for y in firstPostIMG:
    firstPostIMGS.append(y['data-src'])
  imgCount = 1
  for y in firstPostIMGS:
    imgData = requests.get(y).content
    fileName = firstPostInfo["Thread-ID"] + "-" + str(imgCount) + ".jpg"
    f = open(fileName, 'wb')
    f.write(imgData)
    f.close()
    imgCount += 1
  firstPostInfo["Image_URLs"] = firstPostIMGS
  if (len(firstPostIMGS) > 0):
    imgCount += 1
  firstPostReplies = {}
  replies = linkSoup.find_all('div', {'class':[classReply]})

  for y in replies:
    replyPost = y.find_all('p')
    replyPostOffset = 0
    replyUser = ""
    if y.find('h2'):
      replyUser = y.find('h2').text.strip()
    else:
      replyUser = "Patient"
      replyPostOffset = 2
    replyDate = replyPost[-1].text.strip()
    allCounter += 1
    answersCounter += 1
    replyPostInfo = {}
    replyID = str(postsCounter) + "-" + str(answersCounter)
    replyPostTexts = []
    textToAdd = replyPost[1+replyPostOffset].text.strip()
    replyPostTexts.append(textToAdd)
    totalPost += len(textToAdd.split())
    """
    replyPostInfo["Topic"] = titles
    titleSet.add(replyPostInfo["Topic"])
    if (replyPostInfo["Topic"] in titlePost.keys()):

    else:
      titlePost[replyPostInfo["Topic"]] = 1
    """
    titlePost[titles] += 1
#    replyPostInfo["Date"] = replyDate
    replyPostInfo["User-ID"] = replyUser
    replyPostInfo["Text"] = replyPostTexts
#    replyPostInfo["URL"] = x
    firstPostReplies[replyID] = replyPostInfo
    replyReplies = y.find_all(class_=classReply2)

    tempCounter = answersCounter
    for z in replyReplies:
      secondReplyPost = z.find_all('p')
      secondUser = z.find(class_=classUsers).text.strip()
      secondDate = secondReplyPost[-1].text.strip()
      tempCounter += 0.001
      allCounter += 1
      secondPostInfo = {}
      secondPostInfo["User-ID"] = secondUser
      secondReplyID = str(postsCounter) + "-" + str(round(tempCounter,3))
      secondPostTexts = []
      secondTextToAdd = secondReplyPost[-2].text.strip()
      secondPostTexts.append(secondTextToAdd)
      totalPost += len(secondTextToAdd.split())
#      secondPostInfo["Topic"] = titles
      titlePost[titles] += 1
#      secondPostInfo["Date"] = secondDate
      secondPostInfo["Text"] = secondPostTexts
#      secondPostInfo["URL"] = x
      firstPostReplies[secondReplyID] = secondPostInfo
  firstPostInfo["Number_Of_Replies"] = len(firstPostReplies)
  firstPostInfo["Replies"] = firstPostReplies
  data[allCounter] = firstPostInfo



sortedTitles = dict(sorted(titlePost.items(), key=lambda item: item[1], reverse=True))
totalTitles = 0
for titles in list(sortedTitles.values()):
  totalTitles += titles

with open('MarhamResults1.txt', 'w') as file:
  file.write("The total amount of links searched through is: " + str(totalPostCounter) + "\n")
  file.write("The total amount of posts is: " + str(allCounter) + "\n")
  file.write("The amount of unique titles is: " +  str(len(titleSet)) + "\n")
  file.write("The total amount of posts with images is: " +  str(imgCount) + "\n")
  file.write("The average amount of posts with images is: " + str(imgCount/totalPostCounter) + "\n")
  file.write("The highest amount of posts under one title is: " + str(list(sortedTitles.values())[0]) + ", under the title: " + str(list(sortedTitles.keys())[0]) + "\n")
  file.write("The average length of a post in words is: " + str(totalPost/len(data.values())) + "\n")
  file.write("The average posts under a title is: " + str(totalTitles/len(titleSet)) + "\n")

actualData = list(data.values())
jsonData = json.dumps(actualData, separators=(",\n", " : "), indent=4)
with open('MarhamWebscrape1.json', 'w') as file:
    file.write(jsonData)

