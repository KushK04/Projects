# -*- coding: utf-8 -*-
"""Webscrape1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hEaP2CsFQacNfbBVbKwF1MYgRiHSEx6l
"""

from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
import json

#Information on the classes we need
link = "https://www.mumsnet.com/talk/general_health?page=" #Generic link for forum
classOP = "mt-2.5 leading-6 prose prose-bullets-dark w-full break-words dark:text-white" #class for post text
classPosts = "text-blue-700 hover:underline focus:underline focus:outline-none cursor-pointer visited:text-purple-700 dark:visited:text-purple-500 lg:pr-5 dark:text-white" #class for post links
classReply = "lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 overflow-x-hidden bg-white dark:bg-gray-800 border-gray-200" #class for replies
classReply2 = "lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 overflow-x-hidden bg-mumsnet-forest dark:bg-mumsnet-forest-dark border-mumsnet-forest-border"
classTitle = "inline text-2xl font-bold break-words h4" #class for titles
classFirstPost = "p-4 pb-1 pt-2.5 lg:py-2.5 mt-2.5 lg:mt-1.5 border-t border-b sm:border sm:rounded border-mumsnet-forest-border bg-mumsnet-forest dark:bg-mumsnet-forest-dark" #class for the first post
classOUser = "mt-1 lg:mt-1.5 font-bold break-all" #class for original user
classUsers = "font-bold break-all mt-2"#class for all replying users
classReplyLength = "flex items-center justify-end lg:flex-1 space-x-3 lg:m-0 lg:space-x-2" #class for length of reply pages
classLinkPic = "object-cover w-full"

data = {}
linkList = [] #List of topic links
totalPost = 0
titleSet = set()
titlePost = {}
imgCount = 0

print("Gathering Links")

#Gets the links in linkList
start = 2
end = 500
for y in range(start,end): #5:54:47 for 500 Pages
  print(y, " / ", end)
  linkNew = link + str(y)
  linkRequest = requests.get(linkNew)
  linkSoup = BeautifulSoup(linkRequest.text,'lxml')
  links = linkSoup.find_all('a', href=True, class_=classPosts)
  for x in links:
    linkRequestPrev = requests.get(x['href'])
    linkSoupPrev = BeautifulSoup(linkRequestPrev.text,'lxml')
    if(linkSoupPrev.find('img', class_=classLinkPic)):
      linkList.append(x['href'] + "?page=")

print("Iterating through Links")

counter = 0
for x in linkList: #Iterating through each webpage
  currentLink = linkList.index(x)
  print(currentLink, " / ", len(linkList))
  counter += 1
  linkRequest = requests.get(x)
  linkSoup = BeautifulSoup(linkRequest.text,'lxml')
  if(linkSoup.find(class_=classReplyLength) != None and x[-6:] == "?page="):
    appendCurrentLink = 1
    for l in range(int(re.findall(r'\d+', linkSoup.find(class_=classReplyLength).text)[-1]) - 1):
      linkList.insert(currentLink + appendCurrentLink, x+str(l+2))
      appendCurrentLink += 1
  titles = linkSoup.find_all('h1', class_=classTitle)
  firstPost = linkSoup.find(class_=classFirstPost).find_all('p')
  firstPostUser = firstPost[0].text.split("·")[0].replace('\n','').strip()
  firstPostDate = firstPost[0].text.split("·")[1].replace('\n','').strip()
  firstPostInfo = {}
  firstPostIMG = linkSoup.find(class_=classFirstPost).find_all('img')
  firstPostInfo["User-ID"] = firstPostUser
  firstPostInfo["Post-ID"] = counter
  firstPostTexts = []
  for p in firstPost:
    if(p.parent.get("class") == classOP.split(" ")):
       textToAdd = p.text.strip()
       firstPostTexts.append(textToAdd)
       totalPost += len(textToAdd.split())
  firstPostInfo["Topic"] = titles[0].text.replace('\n','').strip()
  titleSet.add(firstPostInfo["Topic"])
  if (firstPostInfo["Topic"] in titlePost.keys()):
    titlePost[firstPostInfo["Topic"]] += 1
  else:
    titlePost[firstPostInfo["Topic"]] = 1
  firstPostInfo["Date"] = firstPostDate
  firstPostInfo["Texts"] = firstPostTexts
  firstPostIMGS = []
  for y in firstPostIMG:
    firstPostIMGS.append(y['src'])
  firstPostInfo["Images"] = firstPostIMGS
  if (len(firstPostIMGS) > 0):
    imgCount += 1
  data[firstPostDate] = firstPostInfo

  replies = linkSoup.find_all('div', {'class':[classReply, classReply2]})
  for y in replies:
    replyPost = y.find_all('p')
    replyUser = replyPost[0].text.split("·")[0].replace('\n','').strip()
    replyDate = replyPost[0].text.split("·")[1].replace('\n','').strip()
    temp = 0
    if(len(replyPost) > 3 and "·" in replyPost[1].text and replyPost[-1].text[-38:] != "Message withdrawn at poster's request."):
      temp = counter
      counter = data[replyPost[1].text.split("·")[1].replace('\n','').strip()]["Post-ID"]
      counter += 0.001
      counter = round(counter, 3)
    else:
      counter += 1
    replyPostInfo = {}
    replyPostIMG = y.find_all('img')
    replyPostInfo["User-ID"] = replyUser
    replyPostInfo["Post-ID"] = counter
    replyPostTexts = []
    for p2 in replyPost:
      if(p2.parent.get("class") == classOP.split(" ")):
        textToAdd = p2.text.strip()
        replyPostTexts.append(p2.text.strip())
        totalPost += len(textToAdd.split())
    replyPostInfo["Topic"] = titles[0].text.replace('\n','').strip()
    titleSet.add(replyPostInfo["Topic"])
    if (replyPostInfo["Topic"] in titlePost.keys()):
      titlePost[replyPostInfo["Topic"]] += 1
    else:
      titlePost[replyPostInfo["Topic"]] = 1
    replyPostInfo["Date"] = replyDate
    replyPostInfo["Texts"] = replyPostTexts
    replyPostIMGS = []
    for z in replyPostIMG:
      replyPostIMGS.append(z['src'])
    replyPostInfo["Images"] = replyPostIMGS
    if (len(replyPostIMGS) > 0):
      imgCount += 1
    data[replyPost[0].text.split("·")[1].replace('\n','').strip()] = replyPostInfo
    if(len(replyPost) > 2 and temp != 0):
      counter = temp

sortedTitles = dict(sorted(titlePost.items(), key=lambda item: item[1], reverse=True))
totalTitles = 0
for titles in list(sortedTitles.values()):
  totalTitles += titles

with open('results1.txt', 'w') as file:
  file.write("The total amount of posts is: " + str(counter) + "\n")
  file.write("The amount of unique titles is: " +  str(len(titleSet)) + "\n")
  file.write("The total amount of posts with images is: " +  str(imgCount) + "\n")
  file.write("The average amount of posts with images is: " + str(imgCount/counter) + "\n")
  file.write("The highest amount of posts under one title is: " + str(list(sortedTitles.values())[0]) + ", under the title: " + str(list(sortedTitles.keys())[0]) + "\n")
  file.write("The average length of a post in words is: " + str(totalPost/len(data.values())) + "\n")
  file.write("The average posts under a title is: " + str(totalTitles/len(titleSet)) + "\n")

actualData = list(data.values())
jsonData = json.dumps(actualData, separators=(",\n", " : "), indent=4)
with open('webscrape1.json', 'w') as file:
    file.write(jsonData)
